{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/geremiapompei/Desktop/Progetti/energiai\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "# !git clone https://github.com/GeremiaPompei/energiai\n",
    "# %cd energiai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from src.utility.hyperparams_generator import gridsearch_generator\n",
    "from src.runner.model_selection_pipeline import model_selection_pipeline\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gridsearch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "hyperparams_list = gridsearch_generator(\n",
    "    dict(\n",
    "        trainer_lr=[0.001, 0.1],\n",
    "        model_hidden_dim=[400],\n",
    "        model_latent_dim=[200],\n",
    "    )\n",
    ")\n",
    "model_selection_pipeline(hyperparams_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Randomsearch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-31 10:24:11,699 - root - INFO - Start iteration 1/100 => hyperparams: {'trainer_lr': 0.0026106005498359943, 'model_hidden_dim': 340, 'model_latent_dim': 310}\n",
      "2023-10-31 10:24:23,949 - root - INFO - Epoch 1/20 => training loss: 1039080.4879624664, test loss: 0.025316547312058894\n",
      "2023-10-31 10:24:35,382 - root - INFO - Epoch 2/20 => training loss: 8229.908339659522, test loss: 0.006489044800464311\n",
      "2023-10-31 10:24:46,800 - root - INFO - Epoch 3/20 => training loss: 3900.1197330780888, test loss: 0.007803047281218021\n",
      "2023-10-31 10:24:58,261 - root - INFO - Epoch 4/20 => training loss: 2684.600362714709, test loss: 0.004235247506763911\n",
      "2023-10-31 10:25:09,690 - root - INFO - Epoch 5/20 => training loss: 2190.984435741814, test loss: 0.002483408775641613\n",
      "2023-10-31 10:25:21,397 - root - INFO - Epoch 6/20 => training loss: 1974.621772854446, test loss: 0.0011408035369070528\n",
      "2023-10-31 10:25:33,113 - root - INFO - Epoch 7/20 => training loss: 1869.9641683848909, test loss: 0.000801777702281651\n",
      "2023-10-31 10:25:45,069 - root - INFO - Epoch 8/20 => training loss: 1824.937079087902, test loss: 0.00036240558004054657\n",
      "2023-10-31 10:25:56,677 - root - INFO - Epoch 9/20 => training loss: 1803.718786224447, test loss: 0.0002017402051621072\n",
      "2023-10-31 10:26:08,475 - root - INFO - Epoch 10/20 => training loss: 1794.0764504759372, test loss: 0.00011190952082068553\n",
      "2023-10-31 10:26:20,151 - root - INFO - Epoch 11/20 => training loss: 1789.241710778161, test loss: 6.724363830630707e-05\n",
      "2023-10-31 10:26:31,777 - root - INFO - Epoch 12/20 => training loss: 1786.7246618062736, test loss: 4.912921154176619e-05\n",
      "2023-10-31 10:26:43,447 - root - INFO - Epoch 13/20 => training loss: 1785.3361396368803, test loss: 4.028149599824423e-05\n",
      "2023-10-31 10:26:55,222 - root - INFO - Epoch 14/20 => training loss: 1784.4527789133822, test loss: 3.7975568443860005e-05\n",
      "2023-10-31 10:27:06,920 - root - INFO - Epoch 15/20 => training loss: 1783.8051582654807, test loss: 3.6391788204679054e-05\n",
      "2023-10-31 10:27:18,532 - root - INFO - Epoch 16/20 => training loss: 1783.2308548232133, test loss: 3.5489002247854685e-05\n",
      "2023-10-31 10:27:29,978 - root - INFO - Epoch 17/20 => training loss: 1782.7040864555456, test loss: 3.508459355132474e-05\n",
      "2023-10-31 10:27:41,586 - root - INFO - Epoch 18/20 => training loss: 1782.1932616696981, test loss: 3.492534707742373e-05\n",
      "2023-10-31 10:27:52,986 - root - INFO - Epoch 19/20 => training loss: 1781.722059655166, test loss: 3.489496659462318e-05\n",
      "2023-10-31 10:28:05,011 - root - INFO - Epoch 20/20 => training loss: 1781.3364803520763, test loss: 3.4768099037930156e-05\n",
      "2023-10-31 10:28:05,033 - root - INFO - Best loss: 3.4768099037930156e-05, best hyperparams: {'trainer_lr': 0.0026106005498359943, 'model_hidden_dim': 340, 'model_latent_dim': 310}\n",
      "2023-10-31 10:28:05,033 - root - INFO - Start iteration 2/100 => hyperparams: {'trainer_lr': 0.0034048469307695717, 'model_hidden_dim': 148, 'model_latent_dim': 258}\n",
      "2023-10-31 10:28:09,095 - root - INFO - Epoch 1/20 => training loss: 4.3606286530388465e+61, test loss: 0.10936683195205134\n",
      "2023-10-31 10:28:13,102 - root - INFO - Epoch 2/20 => training loss: 231238.82988445577, test loss: 0.05252572856625812\n",
      "2023-10-31 10:28:17,047 - root - INFO - Epoch 3/20 => training loss: 172656.05768797363, test loss: 0.03589205786096452\n",
      "2023-10-31 10:28:20,940 - root - INFO - Epoch 4/20 => training loss: 147931.14278591968, test loss: 0.03829000204069083\n",
      "2023-10-31 10:28:24,862 - root - INFO - Epoch 5/20 => training loss: 139104.1881012927, test loss: 0.03032836811747524\n",
      "2023-10-31 10:28:28,754 - root - INFO - Epoch 6/20 => training loss: 138085.42839565355, test loss: 0.029132102284402803\n",
      "2023-10-31 10:28:32,643 - root - INFO - Epoch 7/20 => training loss: 133848.99595728994, test loss: 0.028679075725272778\n",
      "2023-10-31 10:28:36,658 - root - INFO - Epoch 8/20 => training loss: 126067.79853756053, test loss: 0.025582059733601496\n",
      "2023-10-31 10:28:40,686 - root - INFO - Epoch 9/20 => training loss: 119357.69197222943, test loss: 0.023803148302378216\n",
      "2023-10-31 10:28:44,720 - root - INFO - Epoch 10/20 => training loss: 118204.0037171402, test loss: 0.019285958505903346\n",
      "2023-10-31 10:28:48,637 - root - INFO - Epoch 11/20 => training loss: 2702829.4650500626, test loss: 0.023332763168056388\n",
      "2023-10-31 10:28:52,528 - root - INFO - Epoch 12/20 => training loss: 4972411.637155803, test loss: 0.022473590096941744\n",
      "2023-10-31 10:28:56,514 - root - INFO - Epoch 13/20 => training loss: 116572.01701258373, test loss: 0.021800670301458214\n",
      "2023-10-31 10:29:00,426 - root - INFO - Epoch 14/20 => training loss: 115523.76267281557, test loss: 0.023177952542636385\n",
      "2023-10-31 10:29:04,406 - root - INFO - Epoch 15/20 => training loss: 113341.08570189588, test loss: 0.021962349145670648\n",
      "2023-10-31 10:29:08,442 - root - INFO - Epoch 16/20 => training loss: 114128.24231279615, test loss: 0.02288698213373885\n",
      "2023-10-31 10:29:12,390 - root - INFO - Epoch 17/20 => training loss: 115553.02167389586, test loss: 0.022206239910593617\n",
      "2023-10-31 10:29:16,482 - root - INFO - Epoch 18/20 => training loss: 111937.34021295208, test loss: 0.02167584004985436\n",
      "2023-10-31 10:29:20,637 - root - INFO - Epoch 19/20 => training loss: 112472.11506732427, test loss: 0.021025294679645957\n",
      "2023-10-31 10:29:24,633 - root - INFO - Epoch 20/20 => training loss: 111703.37345792679, test loss: 0.021277672699223205\n",
      "2023-10-31 10:29:24,641 - root - INFO - Start iteration 3/100 => hyperparams: {'trainer_lr': 0.001237897305602952, 'model_hidden_dim': 321, 'model_latent_dim': 112}\n",
      "2023-10-31 10:29:32,302 - root - INFO - Epoch 1/20 => training loss: 1.441470195634487e+17, test loss: 0.019131364937810376\n",
      "2023-10-31 10:29:39,754 - root - INFO - Epoch 2/20 => training loss: 60278.27225544485, test loss: 0.014067138508871226\n",
      "2023-10-31 10:29:47,185 - root - INFO - Epoch 3/20 => training loss: 71850.7514125107, test loss: 0.011439263454874744\n",
      "2023-10-31 10:29:54,665 - root - INFO - Epoch 4/20 => training loss: 74074.49499254013, test loss: 0.011771171072474558\n",
      "2023-10-31 10:30:02,173 - root - INFO - Epoch 5/20 => training loss: 78495.5362350407, test loss: 0.012164053716353974\n",
      "2023-10-31 10:30:09,839 - root - INFO - Epoch 6/20 => training loss: 78213.52379005942, test loss: 0.01211233093634451\n",
      "2023-10-31 10:30:17,602 - root - INFO - Epoch 7/20 => training loss: 79979.51474536429, test loss: 0.012508348161897952\n",
      "2023-10-31 10:30:25,360 - root - INFO - Epoch 8/20 => training loss: 79383.82679909558, test loss: 0.013272513327503805\n",
      "2023-10-31 10:30:33,087 - root - INFO - Epoch 9/20 => training loss: 76666.57601944506, test loss: 0.01211097758575072\n",
      "2023-10-31 10:30:40,670 - root - INFO - Epoch 10/20 => training loss: 468762.8042008312, test loss: 0.012962780074158135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m hyperparams_list \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m      3\u001B[0m         trainer_lr\u001B[38;5;241m=\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0.01\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m     ) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m      7\u001B[0m ]\n\u001B[0;32m----> 8\u001B[0m \u001B[43mmodel_selection_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhyperparams_list\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Progetti/energiai/src/runner/model_selection_pipeline.py:42\u001B[0m, in \u001B[0;36mmodel_selection_pipeline\u001B[0;34m(hyperparams_list, epochs, batch_size, shuffle)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# trainer\u001B[39;00m\n\u001B[1;32m     40\u001B[0m trainer \u001B[38;5;241m=\u001B[39m VAETrainer(model, tr_dataloader, vl_dataloader, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m---> 42\u001B[0m tr_loss, vl_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrainer_hyperparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m best_loss \u001B[38;5;241m>\u001B[39m vl_loss:\n\u001B[1;32m     45\u001B[0m     best_hyperparams \u001B[38;5;241m=\u001B[39m hyperparams\n",
      "File \u001B[0;32m~/Desktop/Progetti/energiai/src/trainer/vae_trainer.py:32\u001B[0m, in \u001B[0;36mVAETrainer.__call__\u001B[0;34m(self, epochs, lr)\u001B[0m\n\u001B[1;32m     30\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__loss_function__(x, x_hat, mean, log_var)\n\u001B[1;32m     31\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m---> 32\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     34\u001B[0m tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtr_loader)\n",
      "File \u001B[0;32m~/Desktop/Progetti/energiai/venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Progetti/energiai/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams_list = [\n",
    "    dict(\n",
    "        trainer_lr=random.uniform(0, 0.01),\n",
    "        model_hidden_dim=random.randint(100, 400),\n",
    "        model_latent_dim=random.randint(100, 400),\n",
    "    ) for _ in range(100)\n",
    "]\n",
    "model_selection_pipeline(hyperparams_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
